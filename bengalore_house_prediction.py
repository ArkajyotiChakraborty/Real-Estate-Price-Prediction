# -*- coding: utf-8 -*-
"""Bengalore_House_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QlbrfXl9DBHM45fcDAxbtvFdzFyDWIOB

Name: Arkajyoti Chakraborty 

Branch: Engineering Physics

College :  Delhi Technological University(DTU)
"""

# price is the our dependent variable and rest are independent variable hence we will be predicting the price based on the given condition and also the price is given in indian lakh rupees.
# here we are using supervised learning model and hence here the train and target value all are given to us and based on that we will be bulding our model.

# Commented out IPython magic to ensure Python compatibility.
import numpy as np 
import pandas as pd
from matplotlib import pyplot as plt
# %matplotlib inline
import matplotlib
matplotlib.rcParams["figure.figsize"] = (20,10)

"""DATA Cleaning """

url = 'https://raw.githubusercontent.com/ArkajyotiChakraborty/Real-Estate-Price-Prediction/master/Bengaluru_House_Data.csv'
df = pd.read_csv(url)
df.head()

df

df.shape

df.groupby('area_type')['area_type'].agg('count')

# Removing the Col that are not of our use such as the avilability.
df_new = df.drop(['area_type', 'availability', 'society','balcony'], axis='columns')
df_new

# Till now we have seen that we have created a data set that contains all the vital information that are required to make prediction and build the model.

"""**DATA CLEANING Process..**"""

# Checking the values which are null or not having any values in our data set. 
df_new.isnull().sum() # Number of Rows with NUll Value.

# Here we are facing that 73 rows of bath and some 16 rows of size. Now if u see there's are some 13320 rows and in respect to 13320 rows there are very few rows that are null and then it won't matter much if we drop them. 
df_perfect = df_new.dropna()
df_perfect.isnull().sum()
# here we see that our data set df_perfect that contains no null values.

df_perfect.shape

df_perfect['size'].unique() # Now as we are exploring the size data we can see that 4 bedrooms and 4 BHK are the same thing, hence we can create a col that only contains the BHK thing as ultimately it means the same.

df_perfect

# creating a new col:
df_perfect['bhk'] = df_perfect['size'].apply(lambda x: int(x.split(' ')[0]))

df_perfect # here see the difference that we have an extra col that indicates the BHK of the house.

df_perfect['bhk'].unique() # here we see the numbers and range of bedrooms offered to us in the data sets.

df_perfect['total_sqft'].unique()

# '1133 - 1384' in the above cell we can see that range of numbers and we can change this range of numbers so now we are going to tackle that part only.

# we will be defining an function that converts the values in float and if something of the range as given above is thrown to us we will return false. 
def is_float(x):
    try:
        float(x)
    except:
        return False
    return True

df_perfect[~df_perfect['total_sqft'].apply(is_float)] # here we basically got all the values that are given in range.

df_perfect[~df_perfect['total_sqft'].apply(is_float)].head(10) # this also shows the data is fully mixed with random types of values. So now what is the solution to this we will be taking avg of the values that are given as range and hence we can tackle these values. for the values that are given like 34.46 Sq.meter we will neglect these values.

def convert_sqft_to_num(x):
    tokens = x.split('-')
    if len(tokens) == 2:
        return (float(tokens[0])+float(tokens[1]))/2
    try:
        return float(x)
    except:
        return None

# we can test our function as well 
print(convert_sqft_to_num('2166'))

print(convert_sqft_to_num('2100 - 2850'))

print(convert_sqft_to_num('4125Perch'))

df4 = df_perfect.copy()

df4['total_sqft'] = df4['total_sqft'].apply(convert_sqft_to_num)
df4

"""**Feature Engineering**"""

df5 = df4.copy()
df5['price_per_sqft'] = df5['price']*100000/df5['total_sqft'] # creating a column of price per sqft. 
df5

"""
**Dimensionality Reduction**

Any location having less than 10 data points should be tagged as "other" location. This way number of categories can be reduced by huge amount. Later on when we do one hot encoding, it will help us with having fewer dummy columns"""

df5['location'].unique()

len(df5['location'].unique()) # here we see that we are having 1304 loactions and dealing with them is little difficult. Now this is called as dimensionality curse and we need to do soemthng to get rid of it.

# let's find out the number of data points for location.
df5['location'] = df5['location'].apply(lambda x: x.strip()) # to remove any spaces that are there in location. 
location_num = df5.groupby('location')['location'].agg('count').sort_values(ascending=False)
location_num

# the purpose of doing this is now we can say after looking at our data that any location that is less than 10 data points will be considered as 'other location'. this shows that the method of dealing dimensionality curse.

len(location_num[location_num<=10])

# therefore there are 1052 data points which are having less than 10 data points out of 1293 data points. 
#list of the locations for the prefred conditions
location_num_less_10 = location_num[location_num<=10]
location_num_less_10

# now we will put these locations as others this is the way to reduce the dimensions.

df5.location = df5.location.apply(lambda x: 'other' if x in location_num_less_10 else x)
len(df5.location.unique())

df5 # in this we can witness the other '13316	other	4 BHK	3600.0	5.0	400.00	4	11111.111111' this one.

"""**Outlier** **Removal**

Outlier are the data errors or the points in the data sets which show the extreme variation. 
"""

# Now here comes the little business logic like simply an ideal way one bedroom can have have sqft of 300 hence this creates an threshhold on which we can depends our data sets. now we will be tackling this problem.

df5[df5.total_sqft/df5.bhk<300].head() # simple thing is see total sqft of 1020 with 6 BHK is little unsual, hence we need to take care of this. simply removing these types of extreme values are required. 

# we call these errors as outliers as we can witness that they show a pretty unsusal values. we can safely remove these values to make our data sets proper.

# One of way the way to removing the outliers. 
df6 =  df5[~(df5.total_sqft/df5.bhk<300)]
print(df6)
print(df6.shape)

df6.price_per_sqft.describe()

# Here we find that min price per sqft is 267 rs/sqft whereas max is 12000000, this shows a wide variation in property prices. We should remove outliers per location using mean and one standard deviation.

# Now here we are going to make a functions that can remove the outliers in the price/Sqft.

def remove_pps_outliers(df):
    df_out = pd.DataFrame()
    for key, subdf in df.groupby('location'):  # grouping data frame by location and then we are using subdf and then finding there meana and standard deviation.
        m = np.mean(subdf.price_per_sqft)      # Mean 
        st = np.std(subdf.price_per_sqft)      # Standard Deviation. 
        reduced_df = subdf[(subdf.price_per_sqft>(m-st)) & (subdf.price_per_sqft<=(m+st))]  # taking those data points based on location that are above the mean- standard deviation and below the mentioned range. 
        df_out = pd.concat([df_out,reduced_df],ignore_index=True)    # concating the required vaariable in the df_out dataframe. 
    return df_out
df7 = remove_pps_outliers(df6) # in df7 we are having the perfect data as of now and we removed the price per sqaurefoot outlier to make this a and store it in a data set.
df7.shape

# Now when we studied our data set we find out that for same SQFT area for three bed room set the price is less than the 2 BHK set. Now this something we can consider as an outlier and needed to be dealt properly. 
# Now this may be because of the locations as well but as of now we dont know anything with full command that whether we are able to do anything in this or not. Hence let's get into visualisations to see throgh the things in perfect order.


def plot_scatter_chart(df,location):
    bhk2 = df[(df.location==location) & (df.bhk==2)]
    bhk3 = df[(df.location==location) & (df.bhk==3)]
    matplotlib.rcParams['figure.figsize'] = (15,10)
    plt.scatter(bhk2.total_sqft,bhk2.price,color='blue',label='2 BHK', s=50)
    plt.scatter(bhk3.total_sqft,bhk3.price,marker='+', color='green',label='3 BHK', s=50)
    plt.xlabel("Total Square Feet Area")
    plt.ylabel("Price (Lakh Indian Rupees)")
    plt.title(location)
    plt.legend()

plot_scatter_chart(df7,"Hebbal")    


# we plotted a scatter plot to show that 2 BHK flats and 3 BHK flats price over a SQFT area points which show that price of 2 BHK is more than 3 BHK is considered as outlier.

plot_scatter_chart(df7,"Rajaji Nagar")

def remove_bhk_outliers(df):
    exclude_indices = np.array([])
    for location, location_df in df.groupby('location'):
        bhk_stats = {}
        for bhk, bhk_df in location_df.groupby('bhk'):
            bhk_stats[bhk] = {
                'mean': np.mean(bhk_df.price_per_sqft),
                'std': np.std(bhk_df.price_per_sqft),
                'count': bhk_df.shape[0]
            }
        for bhk, bhk_df in location_df.groupby('bhk'):
            stats = bhk_stats.get(bhk-1)
            if stats and stats['count']>5:
                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)
    return df.drop(exclude_indices,axis='index')
df8 = remove_bhk_outliers(df7)
df8.shape

plot_scatter_chart(df8,"Hebbal")

plot_scatter_chart(df8,"Rajaji Nagar")

# An visualisation to show that our data set at this point shows to be good enough.
import matplotlib
matplotlib.rcParams["figure.figsize"] = (20,10)
plt.hist(df8.price_per_sqft,rwidth=0.5)
plt.xlabel("Price Per Square Feet")
plt.ylabel("Count")

# Now explore the bathroom feature

df8.bath.unique()

# Now this seems a little wierd right as we have never seen so let's fig that out,.

df8[df8['bath']>10]

# visualising the bathroom range to get an idea of outlining,.
plt.hist(df8.bath,rwidth=0.8)
plt.xlabel("Number of bathrooms")
plt.ylabel("Count")

df8[df8.bath>df8.bhk+2]
# here also we see that it's always quite unusual as having 3 bathrooms in 2 bhk or 4 bathrooms in 4 bhk still seems sensible but here it's not natural to have 7 bathrooms in 4 bhk.

df9 = df8[df8.bath<df8.bhk+2]
df9.shape

# creating a data frame with accurate info and things to work on

df10 = df9.drop(['size','price_per_sqft'],axis='columns')
df10

# new cahllenge here is to convert the text data of the location to converted into numeric values. then our ML model can work on that.

"""# **One Hot Encoding**"""

dummies = pd.get_dummies(df10['location'])

dummies

# concatinating the dataframes

df11 = pd.concat([df10,dummies],axis='columns')
df11.head()

# concatinating the dataframes

df11 = pd.concat([df10,dummies.drop('other',axis='columns')],axis='columns')
df11.head()

# here we are witnessing 247 cols in previous cell and now 246 there we dopped the other we created earlier and now we can easily drop our location cols.

df12 = df11.drop('location', axis='columns')
df12.head()

"""# **Model BUilding Machine Learning**"""

# dropping the price cols that will give me X that gives me all independent variable and hence we can train our model. 
X = df12.drop(['price'],axis='columns')
X.head()

y = df12['price'] # creating Y our dependent or target variable. 

y.head()

# train_ test _ split 
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=10) # 20% model for testing and 80% for training.

# Llinear regression

from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X_train,y_train)
model.score(X_test,y_test)

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import ShuffleSplit

from sklearn.linear_model import Lasso
from sklearn.tree import DecisionTreeRegressor

def find_best_model_using_gridsearchcv(X,y):
    algos = {
        'linear_regression' : {
            'model': LinearRegression(),
            'params': {
                'normalize': [True, False]
            }
        },
        'lasso': {
            'model': Lasso(),
            'params': {
                'alpha': [1,2],
                'selection': ['random', 'cyclic']
            }
        },
        'decision_tree': {
            'model': DecisionTreeRegressor(),
            'params': {
                'criterion' : ['mse','friedman_mse'],
                'splitter': ['best','random']
            }
        }
    }
    scores = []
    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)
    for algo_name, config in algos.items():
        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)
        gs.fit(X,y)
        scores.append({
            'model': algo_name,
            'best_score': gs.best_score_,
            'best_params': gs.best_params_
        })

    return pd.DataFrame(scores,columns=['model','best_score','best_params'])

find_best_model_using_gridsearchcv(X,y)

X.columns

# In the above cell using the hyper-parameter tunning and 'LINEAR REGRESSION is the winner'
def predict_price(location,sqft,bath,bhk):    
    loc_index = np.where(X.columns==location)[0][0]

    x = np.zeros(len(X.columns))
    x[0] = sqft
    x[1] = bath
    x[2] = bhk
    if loc_index >= 0:
        x[loc_index] = 1

    return model.predict([x])[0]

# result time
predict_price('1st Phase JP Nagar',1000, 2, 2)

predict_price('1st Phase JP Nagar',1000, 3, 3)

predict_price('Vishwapriya Layout',1000,3,3)

predict_price('Vishwapriya Layout',1000,2,2)

predict_price('Indira Nagar',1000, 3, 3)

predict_price('Indira Nagar',1000, 2, 2)